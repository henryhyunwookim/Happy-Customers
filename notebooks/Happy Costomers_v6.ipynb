{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>1. Load custom functions</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *\n",
    "random_state = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>2. Load data and perform EDA (Exploratory Data Analysis)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Y  X1  X2  X3  X4  X5  X6\n",
      "0  0   3   3   3   4   2   4\n",
      "1  0   3   2   3   5   4   3\n",
      "2  1   5   3   3   3   3   5\n",
      "3  0   5   4   3   3   3   5\n",
      "4  0   5   4   3   3   3   5 \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 126 entries, 0 to 125\n",
      "Data columns (total 7 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   Y       126 non-null    int64\n",
      " 1   X1      126 non-null    int64\n",
      " 2   X2      126 non-null    int64\n",
      " 3   X3      126 non-null    int64\n",
      " 4   X4      126 non-null    int64\n",
      " 5   X5      126 non-null    int64\n",
      " 6   X6      126 non-null    int64\n",
      "dtypes: int64(7)\n",
      "memory usage: 7.0 KB\n",
      "None \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>126.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>126.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.547619</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>2.531746</td>\n",
       "      <td>3.309524</td>\n",
       "      <td>3.746032</td>\n",
       "      <td>3.650794</td>\n",
       "      <td>4.253968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.499714</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.114892</td>\n",
       "      <td>1.023440</td>\n",
       "      <td>0.875776</td>\n",
       "      <td>1.147641</td>\n",
       "      <td>0.809311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Y          X1          X2          X3          X4          X5  \\\n",
       "count  126.000000  126.000000  126.000000  126.000000  126.000000  126.000000   \n",
       "mean     0.547619    4.333333    2.531746    3.309524    3.746032    3.650794   \n",
       "std      0.499714    0.800000    1.114892    1.023440    0.875776    1.147641   \n",
       "min      0.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "25%      0.000000    4.000000    2.000000    3.000000    3.000000    3.000000   \n",
       "50%      1.000000    5.000000    3.000000    3.000000    4.000000    4.000000   \n",
       "75%      1.000000    5.000000    3.000000    4.000000    4.000000    4.000000   \n",
       "max      1.000000    5.000000    5.000000    5.000000    5.000000    5.000000   \n",
       "\n",
       "               X6  \n",
       "count  126.000000  \n",
       "mean     4.253968  \n",
       "std      0.809311  \n",
       "min      1.000000  \n",
       "25%      4.000000  \n",
       "50%      4.000000  \n",
       "75%      5.000000  \n",
       "max      5.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "survey_df = load_data(file_name=\"ACME-HappinessSurvey2020.csv\")\n",
    "print(survey_df.head(), \"\\n\")\n",
    "print(survey_df.info(), \"\\n\")\n",
    "survey_df.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set is simple - only 126 rows and 6 columns, all integer data type with a range between 0 and 5.<br>\n",
    "<br>\n",
    "Since the target variable Y is a categorical variable (0 - unhappy, 1 - happy), convert it accordingly in the data frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df[\"Y\"] = survey_df[\"Y\"].astype('category').cat.set_categories([0, 1], ordered=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot histograms to understand the distribution of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_histograms(data=survey_df,\n",
    "#                 target=\"Y\", target_figsize=(2,2),\n",
    "#                 dependent_layout=(2,3), dependent_figsize=(8,6))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the target variable Y, the distribution is slightly uneven, with more happy customers in the survey data.<br>\n",
    "<b>To deal with this class imbalance problem, SMOTE (synthetic minority oversampling technique) will be used in the next stages.</b><br>\n",
    "<br>\n",
    "The distribution of each dependent variable is also not uniform. However, it would not matter too much as long as the variables/features can provide predictive power.<br>\n",
    "To better understand these features, perform Chi-square tests."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>3. Feature selection</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chi_independence_df = run_chi_tests(data=survey_df, target=\"Y\", significance_level=0.05,\n",
    "#                                     plot_row=2, plot_col=3, figsize=(8,5))\n",
    "# print(\"----------------------------------------------------------------------------\")\n",
    "# print(\"----------------------------------------------------------------------------\")\n",
    "# print(\"2. Chi-square test of independence\")\n",
    "# print(\"----------------------------------------------------------------------------\")\n",
    "# print(\"----------------------------------------------------------------------------\")\n",
    "# print(\"Table 1. Result of Chi-square test of independence (X1-6 and Y)\")\n",
    "# chi_independence_df.set_index(\"Independent Variable\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Chi-square test of goodness of fit</b><br>\n",
    "- This statistical test is often used to evaluate whether or not sample data is representative of the full population.\n",
    "- The null hypothesis was that there is no significant difference between a variable and its expected frequencies.\n",
    "- As we failed to reject the null hypothesis for all 6 dependent variables, we can consider that they are representative of the population at a significance level (or alpha) of 0.05.\n",
    "\n",
    "<b>Chi-square test of independence</b><br>\n",
    "- This statistical test is used to evaluate whether or not a difference between observed expected data is due to chance. We can consider a relationship between the variables exists when failing to reject the null hypothesis.\n",
    "- The null hypothesis was that a dependent variable X# and the target variable Y are independent of each other.\n",
    "- At a significance level of 0.05, we were able to reject the null hypothesis only for X1.\n",
    "- If we were to tolerate a higher chance of error by increasing the alpha to 0.1, we would be able to reject the null hypothesis for X6 as well, which would mean that X6 and Y are not independent of each other.\n",
    "- In summary, X2-X5 are independent of Y, meaning they would not be helpful in predicting Y. Whereas X1 would be helpful in predicting Y at a significance level of 0.05 and X6 as well, at a significance level of 0.1.\n",
    "\n",
    "<b>Relationship between target Y and each dependent variable</b><br>\n",
    "- X1 and X5 seem to be good features for training a predictive model based on the line charts above - higher X values (dependent variable) generally correspond to higher Y values (target variable). However, the chi-square test of independence failed to reject the null hypothesis of \"X5 and Y are independent of each other\" when alpha=0.05.\n",
    "- On the contrary, it appears that it would be hard to predict Y based on the other X variables as fluctuations are observed from the line charts, which were confirmed by the chi-square test of independence.\n",
    "\n",
    "<b>Summary and next step</b><br>\n",
    "- <b>X1 (my order was delivered on time) appears to be the most relevant feature to the target Y (customer satisfaction).</b>\n",
    "- However, to build a more robust model, it would be reasonable to use at least 2 features rather than discarding 5 out of 6 features, especially because the data are not complex nor big.\n",
    "- X5 and X6 could be useful in training a predictive model but it is unclear at this stage whether using either or both of them would be better.\n",
    "- <b>As such, perform the chi-square test of independence again</b>, but this time to test whether there is a relationship between X1 and the other dependent variables <b>to determine what features other than X1 can be useful in model training.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chi_independence_df_X1 = run_chi_tests(data=survey_df.drop([\"Y\"], axis=1), target=\"X1\", significance_level=0.05,\n",
    "#                                        plot_row=2, plot_col=3, figsize=(8,5), plot=False,\n",
    "#                                        goodness_of_fit_test=False)\n",
    "# print(\"Table 2. Result of Chi-square test of independence (X1 and X2-6)\")\n",
    "# chi_independence_df_X1.set_index(\"Independent Variable\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X3, X5 and X6 were found to be not independent of X1, meaning they are related to X1.<br>\n",
    "<br>\n",
    "With that, we can now try different combinations of the features (i.e. X1, X3, X5 and X6) to see which combination would result in the best prediction accuracy score.<br>\n",
    "<br>\n",
    "Before doing so, evaludate different classifiers to choose the base model for the next steps."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>4. Model selection</b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, split the data into train and test.<br>\n",
    "<br>\n",
    "As stated above, SMOTE (synthetic minority oversampling technique) will be used for train and test data separately, to handle the class imbalance problem of target Y.<br>\n",
    "<br>\n",
    "Train data will be used for model selection, feature engineering, and hyperparameter tuning.<br>\n",
    "Test data will only be used at the last step to evaluate the fine-tuned model.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split_data(\n",
    "    X=survey_df.drop([\"Y\"], axis=1),\n",
    "    y=survey_df[\"Y\"],\n",
    "    test_size=0.3,\n",
    "    random_state=random_state,\n",
    "    oversampling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_models(X=X_train, y=y_train, n_splits=10, n=100, random_state=random_state)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to be edited\n",
    "(Tried some of the most common classifiers. RandomForestClassifier sometimes outperformed XGBClassifier but XGBClassifier was so much faster and the performance was as good as RandomForestClassifier.\n",
    "(add description of XGB classifier)\n",
    "https://www.nvidia.com/en-us/glossary/data-science/xgboost/)\n",
    "\n",
    "Use this classifier as our base model for the later steps. Moving forward, try different combinations of the features (X1, X3, X5 and X6) to see which combination would result in the best prediction accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_feature_combinations(X=X_train, y=y_train, n_splits=10, n=100, random_state=random_state,\n",
    "#                         classifier=LogisticRegression(random_state=random_state))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>X1 and X3</b> resulted in the best mean prediction accuracy score, thus the other features will not be used from now on.<br>\n",
    "<br>\n",
    "Re-define X_train and X_test with the best features combination for the later steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reduced = X_train[[\"X1\", \"X3\"]]\n",
    "X_test_reduced = X_test[[\"X1\", \"X3\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>5. Feature augmentation</b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the three features, try different data augmentation/transformation techniques that will create new features out of the existing features to see whether they improve prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_transformers(X=X_train_reduced, y=y_train, n_splits=10, n=100, random_state=random_state,\n",
    "#                 classifier=LogisticRegression(random_state=random_state))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the best was RBFSampler -> what it is and why it performs well with the data we have?\n",
    "\n",
    "As we found the mean accuracy scores improved with the transformers, use the best performing transformer for the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer=RBFSampler(random_state=random_state)\n",
    "X_train_transformed = transformer.fit_transform(X_train_reduced)\n",
    "X_test_transformed = transformer.transform(X_test_reduced)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>6. Dimensionality reduction</b>\n",
    "Now we have more features due to the feature augmentation process, try different dimensionality reduction techniques to see if they help improve accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_decomposers(X=X_train_transformed, y=y_train, n_splits=10, n=100, random_state=random_state,\n",
    "#                 classifier=LogisticRegression(random_state=random_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposer = KernelPCA(random_state=random_state)\n",
    "X_train_decomposed = decomposer.fit_transform(X_train_transformed)\n",
    "X_test_decomposed = decomposer.transform(X_test_transformed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decomposers were not helpful in improving accuracy, thus will not be used in the next steps.<br>\n",
    "<br>\n",
    "Next is hyperparameter tuning for the classifier."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>7. Hyperparameter tuning</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_size: 0.24\n",
      "n_iter: 10, cv: 2, best_score: 0.67, test_score: 0.47\n",
      "n_iter: 10, cv: 3, best_score: 0.58, test_score: 0.71\n",
      "n_iter: 10, cv: 4, best_score: 0.57, test_score: 0.71\n",
      "n_iter: 10, cv: 5, best_score: 0.55, test_score: 0.71\n",
      "n_iter: 10, cv: 6, best_score: 0.55, test_score: 0.71\n",
      "n_iter: 10, cv: 7, best_score: 0.58, test_score: 0.71\n",
      "n_iter: 10, cv: 8, best_score: 0.56, test_score: 0.71\n",
      "n_iter: 10, cv: 9, best_score: 0.56, test_score: 0.71\n",
      "n_iter: 10, cv: 10, best_score: 0.58, test_score: 0.71\n",
      "n_iter: 20, cv: 2, best_score: 0.67, test_score: 0.47\n",
      "n_iter: 20, cv: 3, best_score: 0.61, test_score: 0.65\n",
      "n_iter: 20, cv: 4, best_score: 0.67, test_score: 0.5\n",
      "n_iter: 20, cv: 5, best_score: 0.66, test_score: 0.65\n",
      "n_iter: 20, cv: 6, best_score: 0.57, test_score: 0.65\n",
      "n_iter: 20, cv: 7, best_score: 0.58, test_score: 0.71\n",
      "n_iter: 20, cv: 8, best_score: 0.56, test_score: 0.71\n",
      "n_iter: 20, cv: 9, best_score: 0.56, test_score: 0.71\n",
      "n_iter: 20, cv: 10, best_score: 0.58, test_score: 0.71\n",
      "n_iter: 30, cv: 2, best_score: 0.67, test_score: 0.47\n",
      "n_iter: 30, cv: 3, best_score: 0.61, test_score: 0.65\n",
      "n_iter: 30, cv: 4, best_score: 0.67, test_score: 0.5\n",
      "n_iter: 30, cv: 5, best_score: 0.67, test_score: 0.5\n",
      "n_iter: 30, cv: 6, best_score: 0.57, test_score: 0.65\n",
      "n_iter: 30, cv: 7, best_score: 0.58, test_score: 0.74\n",
      "Test score is higher than 0.70!: 0.7352941176470589\n",
      "\n",
      "n_iter: 30, cv: 8, best_score: 0.57, test_score: 0.62\n",
      "n_iter: 30, cv: 9, best_score: 0.56, test_score: 0.71\n",
      "n_iter: 30, cv: 10, best_score: 0.59, test_score: 0.74\n",
      "Test score is higher than 0.70!: 0.7352941176470589\n",
      "\n",
      "n_iter: 40, cv: 2, best_score: 0.67, test_score: 0.47\n",
      "n_iter: 40, cv: 3, best_score: 0.61, test_score: 0.65\n",
      "n_iter: 40, cv: 4, best_score: 0.67, test_score: 0.5\n",
      "n_iter: 40, cv: 5, best_score: 0.67, test_score: 0.5\n",
      "n_iter: 40, cv: 6, best_score: 0.57, test_score: 0.65\n",
      "n_iter: 40, cv: 7, best_score: 0.59, test_score: 0.74\n",
      "Test score is higher than 0.70!: 0.7352941176470589\n",
      "\n",
      "n_iter: 40, cv: 8, best_score: 0.57, test_score: 0.74\n",
      "Test score is higher than 0.70!: 0.7352941176470589\n",
      "\n",
      "n_iter: 40, cv: 9, best_score: 0.56, test_score: 0.71\n",
      "n_iter: 40, cv: 10, best_score: 0.59, test_score: 0.74\n",
      "Test score is higher than 0.70!: 0.7352941176470589\n",
      "\n",
      "n_iter: 50, cv: 2, best_score: 0.67, test_score: 0.47\n",
      "n_iter: 50, cv: 3, best_score: 0.61, test_score: 0.65\n",
      "n_iter: 50, cv: 4, best_score: 0.67, test_score: 0.5\n",
      "n_iter: 50, cv: 5, best_score: 0.67, test_score: 0.5\n",
      "n_iter: 50, cv: 6, best_score: 0.57, test_score: 0.65\n",
      "n_iter: 50, cv: 7, best_score: 0.59, test_score: 0.74\n",
      "Test score is higher than 0.70!: 0.7352941176470589\n",
      "\n",
      "n_iter: 50, cv: 8, best_score: 0.57, test_score: 0.74\n",
      "Test score is higher than 0.70!: 0.7352941176470589\n",
      "\n",
      "n_iter: 50, cv: 9, best_score: 0.56, test_score: 0.71\n",
      "n_iter: 50, cv: 10, best_score: 0.59, test_score: 0.74\n",
      "Test score is higher than 0.70!: 0.7352941176470589\n",
      "\n",
      "n_iter: 60, cv: 2, best_score: 0.67, test_score: 0.47\n",
      "n_iter: 60, cv: 3, best_score: 0.61, test_score: 0.65\n",
      "n_iter: 60, cv: 4, best_score: 0.67, test_score: 0.5\n",
      "n_iter: 60, cv: 5, best_score: 0.67, test_score: 0.5\n",
      "n_iter: 60, cv: 6, best_score: 0.57, test_score: 0.65\n",
      "n_iter: 60, cv: 7, best_score: 0.59, test_score: 0.74\n",
      "Test score is higher than 0.70!: 0.7352941176470589\n",
      "\n",
      "n_iter: 60, cv: 8, best_score: 0.57, test_score: 0.74\n",
      "Test score is higher than 0.70!: 0.7352941176470589\n",
      "\n",
      "n_iter: 60, cv: 9, best_score: 0.56, test_score: 0.71\n",
      "n_iter: 60, cv: 10, best_score: 0.59, test_score: 0.74\n",
      "Test score is higher than 0.70!: 0.7352941176470589\n",
      "\n",
      "n_iter: 70, cv: 2, best_score: 0.67, test_score: 0.47\n",
      "n_iter: 70, cv: 3, best_score: 0.61, test_score: 0.65\n",
      "n_iter: 70, cv: 4, best_score: 0.67, test_score: 0.5\n",
      "n_iter: 70, cv: 5, best_score: 0.67, test_score: 0.5\n",
      "n_iter: 70, cv: 6, best_score: 0.57, test_score: 0.65\n",
      "n_iter: 70, cv: 7, best_score: 0.59, test_score: 0.74\n",
      "Test score is higher than 0.70!: 0.7352941176470589\n",
      "\n",
      "n_iter: 70, cv: 8, best_score: 0.67, test_score: 0.5\n",
      "n_iter: 70, cv: 9, best_score: 0.56, test_score: 0.71\n",
      "n_iter: 70, cv: 10, best_score: 0.59, test_score: 0.74\n",
      "Test score is higher than 0.70!: 0.7352941176470589\n",
      "\n",
      "n_iter: 80, cv: 2, best_score: 0.67, test_score: 0.47\n",
      "n_iter: 80, cv: 3, best_score: 0.61, test_score: 0.65\n",
      "n_iter: 80, cv: 4, best_score: 0.67, test_score: 0.5\n",
      "n_iter: 80, cv: 5, best_score: 0.67, test_score: 0.5\n",
      "n_iter: 80, cv: 6, best_score: 0.57, test_score: 0.65\n",
      "n_iter: 80, cv: 7, best_score: 0.59, test_score: 0.74\n",
      "Test score is higher than 0.70!: 0.7352941176470589\n",
      "\n",
      "n_iter: 80, cv: 8, best_score: 0.67, test_score: 0.5\n",
      "n_iter: 80, cv: 9, best_score: 0.56, test_score: 0.71\n",
      "n_iter: 80, cv: 10, best_score: 0.59, test_score: 0.74\n",
      "Test score is higher than 0.70!: 0.7352941176470589\n",
      "\n",
      "n_iter: 90, cv: 2, best_score: 0.67, test_score: 0.47\n",
      "n_iter: 90, cv: 3, best_score: 0.61, test_score: 0.65\n",
      "n_iter: 90, cv: 4, best_score: 0.67, test_score: 0.5\n",
      "n_iter: 90, cv: 5, best_score: 0.67, test_score: 0.5\n",
      "n_iter: 90, cv: 6, best_score: 0.57, test_score: 0.65\n",
      "n_iter: 90, cv: 7, best_score: 0.59, test_score: 0.74\n",
      "Test score is higher than 0.70!: 0.7352941176470589\n",
      "\n",
      "n_iter: 90, cv: 8, best_score: 0.67, test_score: 0.5\n",
      "n_iter: 90, cv: 9, best_score: 0.58, test_score: 0.71\n",
      "n_iter: 90, cv: 10, best_score: 0.59, test_score: 0.74\n",
      "Test score is higher than 0.70!: 0.7352941176470589\n",
      "\n",
      "n_iter: 100, cv: 2, best_score: 0.67, test_score: 0.47\n",
      "n_iter: 100, cv: 3, best_score: 0.61, test_score: 0.65\n",
      "n_iter: 100, cv: 4, best_score: 0.67, test_score: 0.5\n",
      "n_iter: 100, cv: 5, best_score: 0.67, test_score: 0.5\n",
      "n_iter: 100, cv: 6, best_score: 0.57, test_score: 0.65\n",
      "n_iter: 100, cv: 7, best_score: 0.59, test_score: 0.74\n",
      "Test score is higher than 0.70!: 0.7352941176470589\n",
      "\n",
      "n_iter: 100, cv: 8, best_score: 0.67, test_score: 0.5\n",
      "n_iter: 100, cv: 9, best_score: 0.58, test_score: 0.71\n",
      "n_iter: 100, cv: 10, best_score: 0.59, test_score: 0.74\n",
      "Test score is higher than 0.70!: 0.7352941176470589\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# n_iter=100\n",
    "# cv = 2\n",
    "# classifier = RandomForestClassifier(random_state=random_state)\n",
    "# params = {\n",
    "#     \"n_estimators\": np.arange(10, 200, 10),\n",
    "#     \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "#     \"max_depth\": [1, 2, 3],\n",
    "#     \"min_samples_split\": [2, 3, 4, 5],\n",
    "#     \"min_samples_leaf\": [1, 2, 3],\n",
    "#     \"max_features\": [\"sqrt\", \"log2\", None],\n",
    "#     \"class_weight\": [\"balanced\", \"balanced_subsample\", None]\n",
    "# }\n",
    "\n",
    "# n_iter=10\n",
    "# cv = 10\n",
    "# for test_size in [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4]:\n",
    "test_size=0.24\n",
    "print(f\"test_size: {test_size}\")\n",
    "X_train, X_test, y_train, y_test = split_data(\n",
    "    X=survey_df.drop([\"Y\"], axis=1),\n",
    "    y=survey_df[\"Y\"],\n",
    "    test_size=test_size,\n",
    "    random_state=random_state,\n",
    "    oversampling=True)\n",
    "\n",
    "X_train_reduced = X_train[[\"X1\", \"X3\"]]\n",
    "X_test_reduced = X_test[[\"X1\", \"X3\"]]\n",
    "\n",
    "transformer=RBFSampler(random_state=random_state)\n",
    "X_train_transformed = transformer.fit_transform(X_train_reduced)\n",
    "X_test_transformed = transformer.transform(X_test_reduced)\n",
    "\n",
    "decomposer = KernelPCA(random_state=random_state)\n",
    "X_train_decomposed = decomposer.fit_transform(X_train_transformed)\n",
    "X_test_decomposed = decomposer.transform(X_test_transformed)\n",
    "\n",
    "for n_iter in np.arange(10, 110, 10):\n",
    "    # print(f\"n_iter: {n_iter}\")\n",
    "    for cv in np.arange(2, 11, 1):\n",
    "        # print(f\"cv: {cv}\")\n",
    "        classifier = LogisticRegression(random_state=random_state)\n",
    "        params = {\n",
    "            \"penalty\": [\"l1\", \"l2\", \"elasticnet\", None],\n",
    "            \"tol\": [10.0 ** n for n in np.arange(-10, 0, 1)],\n",
    "            \"C\": [10.0 ** n for n in np.arange(-10, 0, 1)],\n",
    "            \"fit_intercept\": [True, False],\n",
    "            \"class_weight\": [\"balanced\", None],\n",
    "            \"solver\": ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'],\n",
    "            \"max_iter\": np.arange(10, 200, 10),\n",
    "        }\n",
    "\n",
    "        best_search = RandomizedSearchCV(estimator=classifier,\n",
    "                                        param_distributions=params,\n",
    "                                        scoring=\"f1\",\n",
    "                                        n_iter=n_iter, # n_iter=10 by default\n",
    "                                        random_state=random_state,\n",
    "                                        cv=cv)\n",
    "        best_search.fit(X_train_decomposed, y_train)\n",
    "        # print(best_search.best_estimator_, \"\\n\", best_search.best_score_, \"\\n\", best_search.best_params_)\n",
    "\n",
    "        score = accuracy_score(y_test, best_search.best_estimator_.predict(X_test_decomposed))\n",
    "        print(f\"n_iter: {n_iter}, cv: {cv}, best_score: {round(best_search.best_score_, 2)}, test_score: {round(score, 2)}\")\n",
    "        if score >= 0.73:\n",
    "            print(f\"Test score is higher than 0.70!: {score}\\n\")\n",
    "    # print(score, \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>8. Evaluate the tuned model using test data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score_dict = {}\n",
    "# for random_state in range(100):\n",
    "#     classifier = RandomForestClassifier(random_state=random_state, **best_search.best_params_)\n",
    "#     transformer = RBFSampler(random_state=random_state)\n",
    "#     decomposer = KernelPCA(random_state=random_state)\n",
    "#     clf = make_pipeline(transformer, decomposer, classifier)\n",
    "#     clf.fit(X_train_reduced, y_train)\n",
    "#     score = accuracy_score(y_test, clf.predict(X_test_reduced))\n",
    "#     score_dict[random_state] = score\n",
    "\n",
    "# score_dict_sorted = dict(sorted(score_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "# print(score_dict_sorted[1])\n",
    "# score_dict_sorted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>9. Save the best performing model for future use</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>10. Conclusion</b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learning points, insights, future work, etc.\n",
    "\n",
    "\n",
    "- changed n_splits from 5 to 2 since the data set is so small that the accuracy prediction could not be good when the split was too granular."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a4868653bb6f8972e87e4c446ab8a445a15b25dedb8594cc74c480f8152ea86a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
